"use strict";(self.webpackChunkmodlib_docs=self.webpackChunkmodlib_docs||[]).push([[4447],{3879:(e,s,o)=>{o.r(s),o.d(s,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>d,metadata:()=>n,toc:()=>a});const n=JSON.parse('{"id":"getting_started/custom_models","title":"Custom models","description":"The Application Module Library provides a method to deploy custom-trained models to devices using a similar API to deploying models from the Model Zoo. Due to its modular design, any custom model will work with the already available devices.","source":"@site/_docs_versioned_docs/version-1.0.0/getting_started/custom_models.md","sourceDirName":"getting_started","slug":"/getting_started/custom_models","permalink":"/en/application-module-library/docs/1.0.0/getting_started/custom_models","draft":false,"unlisted":false,"tags":[],"version":"1.0.0","lastUpdatedAt":1750065085000,"sidebarPosition":2,"frontMatter":{"title":"Custom models","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Model zoo","permalink":"/en/application-module-library/docs/1.0.0/getting_started/model_zoo"},"next":{"title":"Devices","permalink":"/en/application-module-library/docs/1.0.0/getting_started/devices"}}');var t=o(4848),i=o(8453),r=o(6036);const d={title:"Custom models",sidebar_position:2},l="Custom models",c={},a=[{value:"Example",id:"example",level:2},{value:"Specification",id:"specification",level:2},{value:"Post Processing",id:"post-processing",level:2},{value:"Pre Processing (Optional)",id:"pre-processing-optional",level:2}];function p(e){const s={admonition:"admonition",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(s.header,{children:(0,t.jsx)(s.h1,{id:"custom-models",children:"Custom models"})}),"\n",(0,t.jsx)(s.p,{children:"The Application Module Library provides a method to deploy custom-trained models to devices using a similar API to deploying models from the Model Zoo. Due to its modular design, any custom model will work with the already available devices."}),"\n",(0,t.jsx)(s.pre,{children:(0,t.jsx)(s.code,{className:"language-python",children:"device = AiCamera()\nmodel = CustomModel()\ndevice.deploy(model)\n"})}),"\n",(0,t.jsx)(s.p,{children:"This means you can easily adapt your custom models, work with various devices, without significant changes to your application code. This approach ensures consistency in your development process, whether you're using models from the zoo or your own custom-trained models."}),"\n",(0,t.jsx)(s.h2,{id:"example",children:"Example"}),"\n",(0,t.jsx)(s.p,{children:"Here's a brief example of how you can use custom models with the Application Module Library:"}),"\n",(0,t.jsx)(s.pre,{children:(0,t.jsx)(s.code,{className:"language-python",children:'from modlib.models import COLOR_FORMAT, MODEL_TYPE, Model\nfrom modlib.models.post_processors import pp_od_bscn\n\nclass SSDMobileNetV2FPNLite320x320(Model):\n    def __init__(self):\n        super().__init__(\n            model_file="./path/to/imx500_network_ssd_mobilenetv2_fpnlite_320x320_pp.rpk",\n            model_type=MODEL_TYPE.RPK_PACKAGED,\n            color_format=COLOR_FORMAT.RGB,\n            preserve_aspect_ratio=True,\n        )\n\n        # Optionally define self.labels\n\n    def post_process(self, output_tensors: List[np.ndarray]) -> Detections:\n        return pp_od_bscn(output_tensors)\n'})}),"\n",(0,t.jsx)(s.h2,{id:"specification",children:"Specification"}),"\n",(0,t.jsxs)(s.p,{children:["To take advantage of modlib's model abstraction layer one needs to follow a certain set of rules.\nThe first one is to initialize the inherited base ",(0,t.jsx)(r.A,{to:"/api-reference/models/model",children:"Model"}),"class."]}),"\n",(0,t.jsx)(s.p,{children:(0,t.jsx)(s.strong,{children:"Arguments:"})}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:"model_file (Path): The path to the model file."}),"\n",(0,t.jsxs)(s.li,{children:["model_type (",(0,t.jsx)(r.A,{to:"/api-reference/models/model#model_type",children:"MODEL_TYPE"}),"): The type of the model."]}),"\n",(0,t.jsxs)(s.li,{children:["color_format (",(0,t.jsx)(r.A,{to:"/api-reference/models/model#color_format",children:"COLOR_FORMAT"}),", optional): The color format of the model (RGB or BGR). Defaults to ",(0,t.jsx)(s.code,{children:"COLOR_FORMAT.RGB"}),"."]}),"\n",(0,t.jsxs)(s.li,{children:["preserve_aspect_ratio (bool, optional): Setting the sensor whether or not to preserve aspect ratio of the input tensor. Defaults to ",(0,t.jsx)(s.code,{children:"True"}),"."]}),"\n"]}),"\n",(0,t.jsxs)(s.admonition,{type:"info",children:[(0,t.jsx)(s.p,{children:"Next to RPK_PACKAGED models, one can also provide a CONVERTED, or quantized KERAS/ONNX models."}),(0,t.jsxs)(s.table,{children:[(0,t.jsx)(s.thead,{children:(0,t.jsxs)(s.tr,{children:[(0,t.jsx)(s.th,{children:"model_type"}),(0,t.jsx)(s.th,{children:"Expected model_file"}),(0,t.jsxs)(s.th,{children:[(0,t.jsx)(s.code,{children:"device.deploy(model)"})," Functionality"]})]})}),(0,t.jsxs)(s.tbody,{children:[(0,t.jsxs)(s.tr,{children:[(0,t.jsx)(s.td,{children:(0,t.jsx)(s.code,{children:"MODEL_TYPE.RPK_PACKAGED"})}),(0,t.jsxs)(s.td,{children:[(0,t.jsx)(s.code,{children:"*.rpk"}),"-file"]}),(0,t.jsx)(s.td,{children:"Uploads packaged model to device."})]}),(0,t.jsxs)(s.tr,{children:[(0,t.jsx)(s.td,{children:(0,t.jsx)(s.code,{children:"MODEL_TYPE.CONVERTED"})}),(0,t.jsxs)(s.td,{children:[(0,t.jsx)(s.code,{children:"packerOut.zip"}),"-file"]}),(0,t.jsx)(s.td,{children:"Packages model for device & uploads."})]}),(0,t.jsxs)(s.tr,{children:[(0,t.jsx)(s.td,{children:(0,t.jsx)(s.code,{children:"MODEL_TYPE.KERAS"})}),(0,t.jsxs)(s.td,{children:[(0,t.jsx)(s.code,{children:"*.keras"}),"-file"]}),(0,t.jsx)(s.td,{children:"Converts quantized model, packages & uploads."})]}),(0,t.jsxs)(s.tr,{children:[(0,t.jsx)(s.td,{children:(0,t.jsx)(s.code,{children:"MODEL_TYPE.ONNX"})}),(0,t.jsxs)(s.td,{children:[(0,t.jsx)(s.code,{children:"*.onnx"}),"-file"]}),(0,t.jsx)(s.td,{children:"Converts quantized model, packages & uploads."})]})]})]})]}),"\n",(0,t.jsx)(s.h2,{id:"post-processing",children:"Post Processing"}),"\n",(0,t.jsx)(s.p,{children:"Implement the necessary post-processing method, which has a strictly defined signature and expected output."}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Argument:"})," output_tensors (",(0,t.jsx)(s.code,{children:"List[np.ndarray]"}),") A list of output tensors returned by your custom model"]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Returns:"})," One of the ",(0,t.jsx)(r.A,{to:"/api-reference/models/results#result",children:"Result"})," types (",(0,t.jsx)(s.code,{children:"Classifications"}),", ",(0,t.jsx)(s.code,{children:"Detections"}),", ",(0,t.jsx)(s.code,{children:"Poses"}),", ",(0,t.jsx)(s.code,{children:"Segments"})," or ",(0,t.jsx)(s.code,{children:"Anomaly"}),")"]}),"\n"]}),"\n",(0,t.jsx)(s.pre,{children:(0,t.jsx)(s.code,{className:"language-python",children:"def post_process(self, output_tensors: List[np.ndarray]) -> Union[Classifications, Detections, Poses, Segments, Anomaly]:\n"})}),"\n",(0,t.jsxs)(s.p,{children:["For convenience, the most common post-processing functions are included in the ",(0,t.jsx)(r.A,{to:"/api-reference/models/post_processors",children:"post_processing library"})," of the Application Module Library."]}),"\n",(0,t.jsx)(s.h2,{id:"pre-processing-optional",children:"Pre Processing (Optional)"}),"\n",(0,t.jsxs)(s.p,{children:["The pre-processing method is only required when deploying the model to an ",(0,t.jsx)("u",{children:"Interpreter Device"})," or when using ",(0,t.jsx)("u",{children:"Data-Injection"}),".\nSimilar to the post-processing, pre-processing has a predifined function signature and expected output."]}),"\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Argument:"})," image (",(0,t.jsx)(s.code,{children:"np.ndarray"}),") The input image of the Source to be processed."]}),"\n",(0,t.jsxs)(s.li,{children:[(0,t.jsx)(s.strong,{children:"Returns:"})," (",(0,t.jsx)(s.code,{children:"Tuple[np.ndarray, np.ndarray]"}),") A tuple (input_tensor_image, input_tensor):","\n",(0,t.jsxs)(s.ul,{children:["\n",(0,t.jsx)(s.li,{children:"Preprocessed input tensor image as a NumPy array."}),"\n",(0,t.jsx)(s.li,{children:"Input tensor ready for model inference."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(s.pre,{children:(0,t.jsx)(s.code,{className:"language-python",children:"def pre_process(self, image: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n"})})]})}function h(e={}){const{wrapper:s}={...(0,i.R)(),...e.components};return s?(0,t.jsx)(s,{...e,children:(0,t.jsx)(p,{...e})}):p(e)}},6036:(e,s,o)=>{o.d(s,{A:()=>d});o(6540);var n=o(8774),t=o(3025),i=o(4070),r=o(4848);function d({to:e,children:s,...o}){const d=(0,t.r)("_docs")?.label||"latest",l=(0,i.ir)("_docs"),c=l?.isLast;let a=e;return e.startsWith("/api-reference/")&&(c||(a=`/api-reference/${d}${e.replace("/api-reference","")}`)),(0,r.jsx)(n.A,{to:a,...o,children:s})}},8453:(e,s,o)=>{o.d(s,{R:()=>r,x:()=>d});var n=o(6540);const t={},i=n.createContext(t);function r(e){const s=n.useContext(i);return n.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function d(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),n.createElement(i.Provider,{value:s},e.children)}}}]);