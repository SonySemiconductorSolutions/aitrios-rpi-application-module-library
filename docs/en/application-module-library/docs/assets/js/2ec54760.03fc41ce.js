"use strict";(self.webpackChunkmodlib_docs=self.webpackChunkmodlib_docs||[]).push([[6289],{8190:e=>{e.exports=JSON.parse('{"version":{"pluginId":"_docs","version":"1.3.0","label":"1.3.0","banner":null,"badge":true,"noIndex":false,"className":"docs-version-1.3.0","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"Intro","href":"/en/application-module-library/docs/","docId":"index","unlisted":false},{"type":"link","label":"Motivation","href":"/en/application-module-library/docs/motivation","docId":"motivation","unlisted":false},{"type":"category","label":"Getting Started","collapsible":true,"collapsed":false,"items":[{"type":"link","label":"Hello world","href":"/en/application-module-library/docs/getting_started/hello_world","docId":"getting_started/hello_world","unlisted":false},{"type":"link","label":"Model zoo","href":"/en/application-module-library/docs/getting_started/model_zoo","docId":"getting_started/model_zoo","unlisted":false},{"type":"link","label":"Custom models","href":"/en/application-module-library/docs/getting_started/custom_models","docId":"getting_started/custom_models","unlisted":false},{"type":"link","label":"Devices","href":"/en/application-module-library/docs/getting_started/devices","docId":"getting_started/devices","unlisted":false}],"href":"/en/application-module-library/docs/category/getting-started"},{"type":"category","label":"Devices","collapsible":true,"collapsed":false,"items":[{"type":"link","label":"AiCamera","href":"/en/application-module-library/docs/devices/ai_camera","docId":"devices/ai_camera","unlisted":false},{"type":"link","label":"Triton\xae Smart","href":"/en/application-module-library/docs/devices/triton","docId":"devices/triton","unlisted":false},{"type":"link","label":"Interpreters","href":"/en/application-module-library/docs/devices/interpreters","docId":"devices/interpreters","unlisted":false}],"href":"/en/application-module-library/docs/category/devices"},{"type":"category","label":"Application Modules","collapsible":true,"collapsed":false,"items":[{"type":"link","label":"Tracker","href":"/en/application-module-library/docs/examples/tracker","docId":"examples/tracker","unlisted":false},{"type":"link","label":"Area","href":"/en/application-module-library/docs/examples/area","docId":"examples/area","unlisted":false},{"type":"link","label":"Counter","href":"/en/application-module-library/docs/examples/counter","docId":"examples/counter","unlisted":false},{"type":"link","label":"Matcher","href":"/en/application-module-library/docs/examples/matcher","docId":"examples/matcher","unlisted":false},{"type":"link","label":"Heatmap","href":"/en/application-module-library/docs/examples/heatmap","docId":"examples/heatmap","unlisted":false},{"type":"link","label":"Calculate","href":"/en/application-module-library/docs/examples/calculate","docId":"examples/calculate","unlisted":false},{"type":"link","label":"Blur","href":"/en/application-module-library/docs/examples/blur","docId":"examples/blur","unlisted":false},{"type":"link","label":"Recording","href":"/en/application-module-library/docs/examples/recording","docId":"examples/recording","unlisted":false},{"type":"link","label":"Motion","href":"/en/application-module-library/docs/examples/motion","docId":"examples/motion","unlisted":false},{"type":"link","label":"Instance Segmentation","href":"/en/application-module-library/docs/examples/instance_segmentation","docId":"examples/instance_segmentation","unlisted":false}],"href":"/en/application-module-library/docs/category/application-modules"},{"type":"link","label":"llms.txt","href":"/en/application-module-library/docs/llms","docId":"llms","unlisted":false},{"type":"link","label":"Release Notes","href":"/en/application-module-library/docs/release_notes","docId":"release_notes","unlisted":false}]},"docs":{"devices/ai_camera":{"id":"devices/ai_camera","title":"AiCamera","description":"Compatible with all Raspberry Pi computers as host, the Raspberry Pi AI Camera takes advantage of Sony\u2019s IMX500 Intelligent Vision Sensor to help you create impressive vision AI applications and neural network models using the on-module AI processor.","sidebar":"tutorialSidebar"},"devices/interpreters":{"id":"devices/interpreters","title":"Interpreters","description":"Interpreter devices allow you to develop and test your application locally on your development PC without requiring a physical camera. This is particularly useful for rapid prototyping, debugging, and testing your AI models and application logic before deploying to a physical device. With interpreter devices, you can use your own image data source and develop your application as if it were connected to a camera image sensor.","sidebar":"tutorialSidebar"},"devices/triton":{"id":"devices/triton","title":"Triton\xae Smart","description":"Installation","sidebar":"tutorialSidebar"},"examples/area":{"id":"examples/area","title":"Area","description":"The Area component allows you to define a polygonal region within a frame by specifying a list of at least three points. Each point is defined as a pair of normalized coordinates [x, y], where x and y are in the range [0, 1], relative to the frame dimensions.","sidebar":"tutorialSidebar"},"examples/blur":{"id":"examples/blur","title":"Blur","description":"The blurring functionality in modLib provides tools for privacy and object anonymization in video streams. You can selectively blur faces or entire objects (like people) in real-time.","sidebar":"tutorialSidebar"},"examples/calculate":{"id":"examples/calculate","title":"Calculate","description":"The Calculate module is designed to contain components focused around calculating values for applications. It contains SpeedCalculator, a class focused on calculating the speed of tracked objects over time by calculating the change in distance over time. Also contains angle calculation, a function to calculate the angle between 3 keypoints values. Finally a few small functions to calculate the distance between two points in 2D pixel space and a function to calculate the center point of a bbox/4 points.","sidebar":"tutorialSidebar"},"examples/counter":{"id":"examples/counter","title":"Counter","description":"The ObjectCounter component is designed to work together with a tracker to keep a persistent count of detected objects over time. By using the tracklet information, it is able to distinguish between different bounding boxes across frames. When a new tracklet is detected, it increments the count for that tracklet\'s associated class.","sidebar":"tutorialSidebar"},"examples/heatmap":{"id":"examples/heatmap","title":"Heatmap","description":"The Heatmap component is used to visualize the density of detected objects across time by overlaying a heatmap onto the frame. The heatmap.update(frame, detections) method stores detections across multiple frames to track the frequency of object appearances in different regions of the frame.","sidebar":"tutorialSidebar"},"examples/instance_segmentation":{"id":"examples/instance_segmentation","title":"Instance Segmentation","description":"The Instance Segmentation component is part of the Segments result class and is designed to identify turn segmentation results into instance segmentation results. It does this by taking the mask of each class and running a Connected Component Analysis (CCA), to get the masks of separated objects. Then to deal with overlapping objects a Watershed algorithm is applied to improve the results of the instance segmentation. From these instance segmentation results you can calculate the bounding boxes or oriented bounding boxes, allowing you to connect Segments to many other modules like Tracker or Matcher.","sidebar":"tutorialSidebar"},"examples/matcher":{"id":"examples/matcher","title":"Matcher","description":"The Matcher component is designed to identify relationships between two sets of detections based on their spatial overlap. It takes two sets of detections and compares their bounding boxes to determine overlaping area. The matcher.match(set1, set2) method returns a boolean mask indicating which detections in the first set overlap with any detection in the second set.","sidebar":"tutorialSidebar"},"examples/motion":{"id":"examples/motion","title":"Motion","description":"The Motion component is designed to identify the change in Motion from frame to frame. It subtracts the current frame from the previous frame to see the change in pixel values. These changes are then turned into bbox results and can be used in combination with other modules.","sidebar":"tutorialSidebar"},"examples/recording":{"id":"examples/recording","title":"Recording","description":"Recording and playback functionality is essential when building applications. It allows you to capture specific scenarios and replay them as many times as needed. This capability enables repeated testing and optimization of your application on a given scenario without requiring live input. Note that the AI detection results are stored along with the frames, ensuring consistent testing of application behavior.","sidebar":"tutorialSidebar"},"examples/tracker":{"id":"examples/tracker","title":"Tracker","description":"An implementation of ByteTrack (GitHub) in the Application Module Library.","sidebar":"tutorialSidebar"},"getting_started/custom_models":{"id":"getting_started/custom_models","title":"Custom models","description":"The Application Module Library provides a method to deploy custom-trained models to devices using a similar API to deploying models from the Model Zoo. Due to its modular design, any custom model will work with the already available devices.","sidebar":"tutorialSidebar"},"getting_started/devices":{"id":"getting_started/devices","title":"Devices","description":"The Application Module Library provides an abstraction layer to work with different devices. In general all devices use a common API:","sidebar":"tutorialSidebar"},"getting_started/hello_world":{"id":"getting_started/hello_world","title":"Hello world","description":"Setup your device","sidebar":"tutorialSidebar"},"getting_started/model_zoo":{"id":"getting_started/model_zoo","title":"Model zoo","description":"The model zoo is currently only compatible with the AiCamera() device in the Application Module Library.","sidebar":"tutorialSidebar"},"index":{"id":"index","title":"Intro","description":"Welcome to the Application Module Library documentation!","sidebar":"tutorialSidebar"},"llms":{"id":"llms","title":"llms.txt","description":"Below you can find the Modlib documentation files in the llms.txt format. This allows large language models (LLMs) and agents to access programming documentation and APIs, particularly useful within integrated development environments (IDEs).","sidebar":"tutorialSidebar"},"motivation":{"id":"motivation","title":"Motivation","description":"Purpose","sidebar":"tutorialSidebar"},"release_notes":{"id":"release_notes","title":"Release Notes","description":"Modlib 1.3.0","sidebar":"tutorialSidebar"}}}}')}}]);